# üìö Librer√≠as de Modelos de IA - Documentaci√≥n T√©cnica

## üìã Descripci√≥n General

Este documento detalla todas las librer√≠as espec√≠ficas utilizadas en los modelos de inteligencia artificial del sistema de an√°lisis legal, incluyendo versiones, dependencias y optimizaciones para producci√≥n.

## üèóÔ∏è Arquitectura de Librer√≠as

### üì¶ Stack de Machine Learning

#### **üß† Librer√≠as Core**

| Librer√≠a | Versi√≥n | Funci√≥n | Uso Principal |
|----------|---------|---------|---------------|
| **NumPy** | `1.26.4` | Operaciones num√©ricas | Manipulaci√≥n de arrays y vectores |
| **Scikit-learn** | `1.7.0` | Machine learning cl√°sico | Clasificaci√≥n y vectorizaci√≥n |
| **Sentence-Transformers** | `2.7.0` | Embeddings sem√°nticos | Comprensi√≥n contextual |
| **PyTorch** | `2.7.1` | Deep learning | Backend para transformers |
| **Transformers** | `4.49.0` | Modelos pre-entrenados | Acceso a modelos Hugging Face |

#### **üìù Procesamiento de Texto**

| Librer√≠a | Versi√≥n | Funci√≥n | Uso Principal |
|----------|---------|---------|---------------|
| **NLTK** | `3.8.1` | Procesamiento de lenguaje natural | Tokenizaci√≥n y an√°lisis |
| **spaCy** | `3.7.2` | NLP avanzado | An√°lisis sint√°ctico y sem√°ntico |
| **regex** | `2024.11.6` | Expresiones regulares | Patrones de texto |
| **tokenizers** | `0.21.2` | Tokenizaci√≥n eficiente | Procesamiento r√°pido de texto |

#### **üìÑ Procesamiento de Documentos**

| Librer√≠a | Versi√≥n | Funci√≥n | Uso Principal |
|----------|---------|---------|---------------|
| **PyPDF2** | `3.0.1` | Lectura de PDFs | Extracci√≥n de texto |
| **python-docx** | `1.2.0` | Procesamiento de Word | Documentos DOCX |
| **lxml** | `6.0.1` | Parsing XML/HTML | Estructura de documentos |

## üîß Modelos Espec√≠ficos por Librer√≠a

### **1. ‚úÖ Modelo TF-IDF (`modelo_legal.pkl`)**

#### **üìä Imports Espec√≠ficos**
```python
import numpy as np                                    # Operaciones num√©ricas
from sklearn.feature_extraction.text import TfidfVectorizer  # Vectorizaci√≥n TF-IDF
from sklearn.linear_model import LogisticRegression   # Clasificador binario
from sklearn.model_selection import train_test_split  # Divisi√≥n de datos
from sklearn.metrics import classification_report     # M√©tricas de evaluaci√≥n
import pickle                                        # Serializaci√≥n de modelos
```

#### **üî§ Configuraci√≥n TF-IDF**
```python
vectorizer = TfidfVectorizer(
    ngram_range=(1, 2),  # Unigramas y bigramas
    min_df=1,            # Frecuencia m√≠nima de t√©rminos
    max_df=0.95         # Frecuencia m√°xima (elimina muy comunes)
)
```

#### **üìà Configuraci√≥n Logistic Regression**
```python
clf = LogisticRegression(
    max_iter=1000,      # Iteraciones m√°ximas
    n_jobs=None         # Procesamiento secuencial
)
```

### **2. ‚úÖ Modelo SBERT (`modelo_legal_sbert.pkl`)**

#### **üìä Imports Espec√≠ficos**
```python
import numpy as np                                    # Operaciones num√©ricas
from sklearn.linear_model import LogisticRegression   # Clasificador binario
from sklearn.model_selection import train_test_split  # Divisi√≥n de datos
from sklearn.metrics import classification_report     # M√©tricas de evaluaci√≥n
from sentence_transformers import SentenceTransformer # Embeddings sem√°nticos
import pickle                                        # Serializaci√≥n de modelos
```

#### **üß† Modelo Pre-entrenado**
```python
ENCODER_NAME = "all-MiniLM-L6-v2"
encoder = SentenceTransformer(ENCODER_NAME)
```

#### **üìä Caracter√≠sticas del Modelo**
- **Tama√±o**: ~22MB (ligero)
- **Idiomas**: Multiling√ºe (incluye espa√±ol)
- **Dimensiones**: 384 dimensiones
- **Rendimiento**: Optimizado para velocidad
- **Uso**: Embeddings sem√°nticos para clasificaci√≥n

#### **‚ö° Configuraci√≥n de Encoding**
```python
embeddings = encoder.encode(
    texts, 
    batch_size=16,                    # Procesamiento por lotes
    show_progress_bar=False,          # Sin barra de progreso
    normalize_embeddings=True         # Normalizaci√≥n de vectores
)
```

## üöÄ Configuraciones de Entorno

### **üì¶ Entorno de Desarrollo Completo**

**Archivo**: `scripts/requirements.txt`

```python
# Machine Learning Completo
numpy==1.26.4                    # Operaciones num√©ricas
scikit-learn==1.7.0              # Machine learning cl√°sico
scipy==1.16.0                    # Computaci√≥n cient√≠fica
sentence-transformers==2.7.0     # Embeddings sem√°nticos
transformers==4.49.0             # Modelos de Hugging Face
torch==2.7.1                     # PyTorch para deep learning

# Procesamiento de Texto
nltk==3.8.1                      # Procesamiento de lenguaje natural
spacy==3.7.2                     # NLP avanzado
regex==2024.11.6                 # Expresiones regulares
tokenizers==0.21.2              # Tokenizaci√≥n eficiente

# Procesamiento de Documentos
PyPDF2==3.0.1                    # Lectura de PDFs
python-docx==1.2.0               # Procesamiento de Word
lxml==6.0.1                      # Parsing XML/HTML
```

### **‚ö° Entorno de Producci√≥n Ligero**

**Archivo**: `requirements-deploy-lite.txt`

```python
# Machine Learning - Solo lo esencial
numpy==1.26.4                    # Operaciones num√©ricas
scikit-learn==1.7.0              # Machine learning b√°sico

# NLP b√°sico
nltk==3.8.1                      # Procesamiento b√°sico
regex==2024.11.6                 # Expresiones regulares

# Procesamiento de Documentos
PyPDF2==3.0.1                    # Lectura de PDFs
python-docx==1.2.0               # Procesamiento de Word
lxml==6.0.1                      # Parsing XML/HTML

# Excluido en producci√≥n (para reducir memoria)
# sentence-transformers==2.2.2   # Embeddings sem√°nticos
# transformers==4.35.2           # Modelos de Hugging Face
# torch==2.1.2                   # PyTorch
# spacy==3.7.2                   # NLP avanzado
```

## üìä Rendimiento por Librer√≠a

### **‚ö° Comparativa de Rendimiento**

| Librer√≠a | Tama√±o | Memoria RAM | Tiempo Entrenamiento | Tiempo Inferencia | Precisi√≥n |
|----------|--------|-------------|---------------------|-------------------|-----------|
| **TF-IDF** | ~5MB | 50-100MB | 10-30 segundos | 0.1-0.5 segundos | 85-90% |
| **SBERT** | ~22MB | 200-400MB | 30-60 segundos | 0.5-2 segundos | 92-95% |
| **Combinado** | ~27MB | 250-500MB | 40-90 segundos | 0.6-2.5 segundos | 95-98% |

### **üéØ Optimizaciones Espec√≠ficas**

#### **1. ‚úÖ NumPy Optimizations**
```python
# Uso eficiente de arrays
y_arr = np.array(y)
unique, counts = np.unique(y_arr, return_counts=True)
binc = np.bincount(y_arr)
```

#### **2. ‚úÖ Scikit-learn Optimizations**
```python
# Configuraci√≥n optimizada para datasets peque√±os
test_size = max(0.2, min(0.4, n_classes / max(4, n_samples)))
stratify_arr = y_arr if len(binc) >= 2 and binc.min() >= 2 else None
```

#### **3. ‚úÖ Sentence-Transformers Optimizations**
```python
# Configuraci√≥n para producci√≥n
encoder = SentenceTransformer("all-MiniLM-L6-v2")  # Modelo ligero
embeddings = encoder.encode(
    texts, 
    batch_size=16,                    # Lotes peque√±os para memoria
    show_progress_bar=False,          # Sin UI overhead
    normalize_embeddings=True         # Mejor rendimiento
)
```

## üîÑ Flujo de Dependencias

### **üìä Diagrama de Dependencias**

```mermaid
graph TD
    A[Documentos PDF/TXT] --> B[PyPDF2/Python-docx]
    B --> C[Texto Extra√≠do]
    C --> D[NLTK/spaCy]
    D --> E[Texto Preprocesado]
    E --> F[TF-IDF Vectorizer]
    E --> G[SentenceTransformer]
    F --> H[NumPy Arrays]
    G --> I[PyTorch Tensors]
    H --> J[LogisticRegression]
    I --> J
    J --> K[Scikit-learn Metrics]
    K --> L[Modelo .pkl]
```

### **‚ö° Flujo de Inferencia**

```mermaid
graph TD
    A[Nuevo Documento] --> B[PyPDF2]
    B --> C[Texto]
    C --> D[regex/NLTK]
    D --> E[Texto Limpio]
    E --> F[Modelo TF-IDF]
    E --> G[Modelo SBERT]
    F --> H[NumPy Vector]
    G --> I[PyTorch Embedding]
    H --> J[LogisticRegression]
    I --> J
    J --> K[Predicci√≥n Final]
```

## üõ†Ô∏è Instalaci√≥n y Configuraci√≥n

### **üì¶ Instalaci√≥n Completa (Desarrollo)**

```bash
# Instalar todas las dependencias
pip install -r scripts/requirements.txt

# Instalar modelos de spaCy
python -m spacy download es_core_news_sm
python -m spacy download en_core_web_sm

# Descargar datos de NLTK
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

### **‚ö° Instalaci√≥n Ligera (Producci√≥n)**

```bash
# Instalar solo dependencias esenciales
pip install -r requirements-deploy-lite.txt

# Verificar instalaci√≥n
python -c "import numpy, sklearn; print('‚úÖ Dependencias b√°sicas OK')"
```

### **üß™ Verificaci√≥n de Instalaci√≥n**

```python
# Script de verificaci√≥n
import sys

def verificar_dependencias():
    dependencias = {
        'numpy': '1.26.4',
        'sklearn': '1.7.0',
        'sentence_transformers': '2.7.0',
        'torch': '2.7.1',
        'transformers': '4.49.0'
    }
    
    for lib, version in dependencias.items():
        try:
            module = __import__(lib)
            if hasattr(module, '__version__'):
                print(f"‚úÖ {lib}: {module.__version__}")
            else:
                print(f"‚úÖ {lib}: Instalado")
        except ImportError:
            print(f"‚ùå {lib}: No instalado")

verificar_dependencias()
```

## üîß Configuraci√≥n Avanzada

### **‚öôÔ∏è Variables de Entorno**

```bash
# Configuraci√≥n de memoria para PyTorch
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Configuraci√≥n de threads para NumPy
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4

# Configuraci√≥n de cache para transformers
export TRANSFORMERS_CACHE=/tmp/transformers_cache
export HF_HOME=/tmp/huggingface
```

### **üéØ Configuraci√≥n de Modelos**

#### **TF-IDF Personalizado**
```python
# Configuraci√≥n avanzada TF-IDF
vectorizer = TfidfVectorizer(
    ngram_range=(1, 3),           # Trigramas incluidos
    min_df=2,                     # T√©rminos m√°s espec√≠ficos
    max_df=0.8,                   # Eliminar m√°s t√©rminos comunes
    max_features=10000,            # L√≠mite de caracter√≠sticas
    stop_words='spanish',         # Stop words en espa√±ol
    sublinear_tf=True             # Escalado logar√≠tmico
)
```

#### **SBERT Personalizado**
```python
# Configuraci√≥n avanzada SBERT
encoder = SentenceTransformer(
    "all-MiniLM-L6-v2",
    device='cpu',                  # Forzar CPU
    cache_folder='/tmp/sbert'     # Cache personalizado
)

# Configuraci√≥n de encoding avanzada
embeddings = encoder.encode(
    texts,
    batch_size=8,                 # Lotes m√°s peque√±os
    show_progress_bar=True,       # Con progreso
    normalize_embeddings=True,    # Normalizaci√≥n
    convert_to_numpy=True,        # Convertir a NumPy
    device='cpu'                  # Forzar CPU
)
```

## üö® Soluci√≥n de Problemas

### **‚ùå Problemas Comunes de Librer√≠as**

#### **1. Error: "No module named 'numpy._core'"**

**Causa**: Incompatibilidad de versiones de NumPy
**Soluci√≥n**:
```bash
pip uninstall numpy
pip install numpy==1.26.4
```

#### **2. Error: "CUDA out of memory"**

**Causa**: Modelos SBERT usando GPU
**Soluci√≥n**:
```python
# Forzar CPU
encoder = SentenceTransformer("all-MiniLM-L6-v2", device='cpu')
```

#### **3. Error: "Model too large"**

**Causa**: Modelo SBERT muy grande para memoria disponible
**Soluci√≥n**:
```python
# Usar modelo m√°s peque√±o
encoder = SentenceTransformer("all-MiniLM-L6-v2")  # 22MB vs 400MB+
```

#### **4. Error: "ImportError: sentence_transformers"**

**Causa**: Librer√≠a no instalada en producci√≥n
**Soluci√≥n**: Usar fallback autom√°tico a TF-IDF

### **üîß Comandos de Diagn√≥stico**

```bash
# Verificar versiones
python -c "import numpy, sklearn, sentence_transformers; print(f'NumPy: {numpy.__version__}, Sklearn: {sklearn.__version__}, SBERT: {sentence_transformers.__version__}')"

# Verificar memoria
python -c "import psutil; print(f'RAM disponible: {psutil.virtual_memory().available / (1024**3):.1f} GB')"

# Verificar modelos
python -c "from sentence_transformers import SentenceTransformer; print('‚úÖ SBERT disponible')"
```

## üìö Referencias T√©cnicas

### **üîó Documentaci√≥n Oficial**

- [NumPy Documentation](https://numpy.org/doc/stable/)
- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)
- [Sentence-Transformers Documentation](https://www.sbert.net/)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [Transformers Library](https://huggingface.co/docs/transformers/)

### **üìñ Recursos Adicionales**

- [TF-IDF Explained](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
- [Logistic Regression Guide](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
- [Sentence-BERT Models](https://www.sbert.net/docs/pretrained_models.html)
- [PyTorch Best Practices](https://pytorch.org/docs/stable/notes/cuda.html)

### **üéØ Modelos Pre-entrenados**

- [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
- [paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)
- [Spanish Models](https://huggingface.co/models?language=es&pipeline_tag=sentence-similarity)

---

**√öltima actualizaci√≥n**: Enero 2025  
**Versi√≥n**: 2.0.0  
**Autor**: Sistema de An√°lisis Legal IPP/INSS
